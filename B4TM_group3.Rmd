---
title: "B4TM_project"
author: "Timo Dijkstra, Tessa Duk, Mickey van Immerseel, Robin Pocornie"
date: "2-4-2021"
knit: (function(input_file, encoding) {
  out_dir <- 'docs';
  rmarkdown::render(input_file,
 encoding=encoding,
 output_file=file.path(dirname(input_file), out_dir, 'index.html'))})
---

# Load packages
```{r}
library("tidyverse")
library("caret")
library("kernlab")
library("caret")
library("rlist")
```

# Load data
Store all the data as tibble. 
Make an additional tibble for the DNA region information. The ID is in the format <chromosome>.<#feature>
```{r}
train_data_array_input <- as.tibble(read.table(file = "Train_call.txt", header = T, sep = "\t"))

target_data_array <- read_tsv(file = "Train_clinical.txt")

Instances <- colnames(train_data_array_input)[-c(1:4)]

feature_information <- as_tibble(t(train_data_array_input[,1:4]))
names(feature_information) <- paste(train_data_array_input$Chromosome, ".", as.character(1:dim(train_data_array_input)[1]), sep = "")

DNA_region_information <- c("Chromosome", "Start", "End", "Nclone")
rownames(feature_information) <- DNA_region_information

```


# Data pre-processing
Transpose the train data matrix.
Remove the DNA region information.
Make the DNA_IDs the new feature names.
```{r}
train_data_array <- as_tibble(t(subset(train_data_array_input, select = -c(Chromosome:Nclone))))
names(train_data_array) <- colnames(feature_information)
train_data_array <-  train_data_array %>% add_column(Instances, .before = "1.1")
```
Add the targets to the train_data_arry. 
For convenience, place the targets in front of the train data set.
```{r}
train_data_array <- train_data_array %>% add_column(target_data_array$Subgroup, .before = "1.1")
train_data_array <- train_data_array %>% rename(Target = "target_data_array$Subgroup")

```


# Feature selection
Split data in train and test data.
Transform the training set to a data frame.
Transform the targets to an object of class "factor".


```{r}
train_data_array_df <- as.data.frame(select(train_data_array, -c(Instances)))
rownames(train_data_array_df) <- Instances
train_data_array_df$Target <- as.factor(train_data_array_df$Target)

validation_index <- createDataPartition(train_data_array_df$Target, p=0.80, list = F)
test_x_df <- train_data_array_df[-validation_index,] %>% select(-Target)
test_y_df <- train_data_array_df[-validation_index,] %>% select(Target)
test_data_array_df <- train_data_array_df[-validation_index,]

train_x_df <- train_data_array_df[validation_index,] %>% select(-Target)
train_y_df <- train_data_array_df[validation_index,] %>% select(Target)
train_data_array_df <- train_data_array_df[validation_index,]
```

Apply the filterVarImp function to determine the most important features.
This gives us a vector of AUC scores for every variable.
Calculate the sum of AUC scores and calculate the sum of squares of AUC scores.
```{r}
rocVarImp <- filterVarImp(train_x_df, train_y_df$Target)
rocVarImp$Sum <- apply(rocVarImp, 1, sum)
rocVarImp$SSQ <- apply(rocVarImp[1:3], 1, function(x){sum(x^2)})
```

## Plot feature importance
Plot the distribution of AUC scores for all the features using the sum of AUCs.
DNA region 17.2185 has the highest score.
http://www.ensembl.org/Homo_sapiens/Location/Overview?r=17:35076296-35282086;db=core
```{r}
auc_scores_per_chromosome <- ggplot(data = rocVarImp, mapping = aes(1:2834, rocVarImp[,4], color = as.factor(train_data_array_input$Chromosome))) +
  geom_point() +
  labs(title = "Feature importance",
       caption = "Sum of AUC scores",
       x = "Feature number",
       y = "Cumulative score")
auc_scores_per_chromosome <- auc_scores_per_chromosome + guides(color=guide_legend(title = "Chromosome"))
auc_scores_per_chromosome 
```

Plot a histogram of AUC scores of the features using the sum of AUCs..
```{r}
auc_scores_histogram <- ggplot(data = rocVarImp, mapping = aes(rocVarImp[,4], color = as.factor(train_data_array_input$Chromosome))) +
  geom_histogram(bins = 30, fill = "white") +
  labs(title = "Feature importance",
       caption = "Sum of AUC scores",
       x = "Cumulative score",
       y = "Frequency")
auc_scores_histogram <- auc_scores_histogram + guides(color=guide_legend(title = "Chromosome"))
auc_scores_histogram 
```

Plot the distribution of AUC scores for all the features using the sum of squares of AUCs.
DNA region 17.2185 has the highest score.
http://www.ensembl.org/Homo_sapiens/Location/Overview?r=17:35076296-35282086;db=core
```{r}
auc_scores_per_chromosome <- ggplot(data = rocVarImp, mapping = aes(1:2834, rocVarImp[,5], color = as.factor(train_data_array_input$Chromosome))) +
  geom_point() +
  labs(title = "Feature importance",
       caption = "Sum of square AUC scores",
       x = "Feature number",
       y = "Cumulative score")
auc_scores_per_chromosome <- auc_scores_per_chromosome + guides(color=guide_legend(title = "Chromosome"))
auc_scores_per_chromosome 
```

Plot a histogram of AUC scores of the features using the sum of squares of AUCs..
```{r}
auc_scores_histogram <- ggplot(data = rocVarImp, mapping = aes(rocVarImp[,5], color = as.factor(train_data_array_input$Chromosome))) +
  geom_histogram(bins = 30, fill = "white") +
  labs(title = "Feature importance",
       caption = "Sum of square AUC scores",
       x = "Cumulative score",
       y = "Frequency")
auc_scores_histogram <- auc_scores_histogram + guides(color=guide_legend(title = "Chromosome"))
auc_scores_histogram 
```

# Cross-validation
Convert the rocVarImp data frame to a decreasing feature order.
```{r}
rocVarImp_descending_sum <- rocVarImp[order(rocVarImp$Sum, decreasing = T),]
rocVarImp_descending_ssq <- rocVarImp[order(rocVarImp$SSQ, decreasing = T),]
```


## kNN model
train_data_array[,c(rownames(rocVarImp_descending_ssq)[1])]
Train a kNN with different training set sizes.
First, define a set of parameters: the seed, number of top features to test, the feature column names plus the target column name to extract, a list that stores the knn models, the cross-validation scheme, the metric for model comparison, the grid search parameters, and the grid search scheme.
Then, determine for every set of features {1, ..., 10} the best model by grid search.
```{r}
set.seed(101)
n_features <- 20
tmp_feature_columns <- c(seq(1,n_features), "Target")
kNN_models <- vector(mode = "list", length = n_features)
control <- trainControl(method = "repeatedcv", number = 5, repeats = 5)
metric <- "Accuracy"
k_grid_search <- seq(3,20)
grid_knn <- expand.grid(k=k_grid_search)


for(i in 1:n_features){
  tmp_feature_columns[i] <- rownames(rocVarImp_descending_ssq)[i]
  fit.knn <- train(Target~., 
                   data=train_data_array_df[,tmp_feature_columns[c(1:i,length(tmp_feature_columns))]], 
                   method="knn", metric=metric, 
                   trControl=control, 
                   tuneGrid = grid_knn)
  kNN_models[[i]] <- fit.knn
}
```


Format the knn_models output
Add an additional column to describe the number of features used in the model.
Make a table with the performances of all the models.
Make a table with the best scoring models.
Expand these tables using a loop and rbind. For the table with all the models, just add the rows to the table. For the best models, order the knn_model results by decreasing accuracy. Then take the first row and add this row to the table.
Plot the performances by ggplot and highlight the best performing models.
```{r, fig.width=10, fig.height=6}
kNN_models[[1]]$results$n_features <- 1
performance_knn <- kNN_models[[1]]$results

best_knn_models <- kNN_models[[1]]$results[order(kNN_models[[1]]$results$Accuracy, decreasing = T),][1,]

for(i in 2:n_features){
  kNN_models[[i]]$results$n_features <- i
  performance_knn <- rbind(performance_knn, kNN_models[[i]]$results)
  tmp_best_model <- kNN_models[[i]]$results[order(kNN_models[[i]]$results$Accuracy, 
                                                  decreasing = T),][1,]
  best_knn_models <- rbind(best_knn_models, tmp_best_model)
}


performance_knn$n_features <- as.factor(performance_knn$n_features)
best_knn_models$n_features <- as.factor(best_knn_models$n_features)

ggplot(data = performance_knn, 
       mapping = aes(x = k, y = Accuracy, color = n_features)) +
  geom_line(size = 1) +
  scale_x_continuous(breaks = k_grid_search) +
  theme_bw() +
  geom_point(data = best_knn_models, 
             mapping = aes(x = k, y = Accuracy, color = n_features), 
             size = 4)

```

Isolate the best performing kNN model
To do this, determine first the best number of features.
```{r}
best_knn_n_features <- best_knn_models[order(best_knn_models$Accuracy, decreasing = T),]$n_features[1]
fit.knn <- kNN_models[[best_knn_n_features]]
fit.knn
```



```{r}
predictions_knn <- predict(fit.knn, test_data_array_df)
confusionMatrix(predictions_knn, test_data_array_df$Target)
```


# Double-loop cross-validation
## kNN model
```{r}
repeats_double_loop_cv <-  20
kNN_models_outer_repeats <- vector(mode = "list", length = repeats_double_loop_cv)

for(n in 0:(repeats_double_loop_cv - 1)){

# Determine the number of outer cross validation loops
cv_outer <- 5

# Initiate a list for the outer loop models.
kNN_models_outer <- vector(mode = "list", length = cv_outer)

# Initialize the start and end test set indices.
test_index_start <- seq(1, dim(train_data_array)[1],)
test_index_end <- seq(dim(train_data_array)[1]/cv_outer, dim(train_data_array)[1] * 2)
test_index_list <- c(seq(1, dim(train_data_array)[1]), seq(1, dim(train_data_array)[1]))

# Outer cross-validation
for(i in seq(1, dim(train_data_array)[1], 20)){
# Define the test and training set in iteration i.
train_data_array_df <- as.data.frame(select(train_data_array, -c(Instances)))
rownames(train_data_array_df) <- Instances
train_data_array_df$Target <- as.factor(train_data_array_df$Target)

test_index <- test_index_list[test_index_start[i + n]:test_index_end[i + n]]

test_x_df <- train_data_array_df[test_index,] %>% select(-Target)
test_y_df <- train_data_array_df[test_index,] %>% select(Target)
test_data_array_df <- train_data_array_df[test_index,]

train_x_df <- train_data_array_df[-test_index,] %>% select(-Target)
train_y_df <- train_data_array_df[-test_index,] %>% select(Target)
train_data_array_df <- train_data_array_df[-test_index,]

# Rank the features on importance based on the filterVarImp function.
rocVarImp <- filterVarImp(train_x_df, train_y_df$Target)
rocVarImp$SSQ <- apply(rocVarImp[1:3], 1, function(x){sum(x^2)})

rocVarImp_descending_ssq <- rocVarImp[order(rocVarImp$SSQ, decreasing = T),]

# Define the model parameters. In general k is sqrt(n) where n is the number of data points. Meaning that 10 would be a reference.
set.seed(101)
n_features <- 12
tmp_feature_columns <- c(seq(1,n_features), "Target")
kNN_models <- vector(mode = "list", length = n_features)
control <- trainControl(method = "repeatedcv", number = 5, repeats = 5)
metric <- "Accuracy"
k_grid_search <- seq(5,15)
grid_knn <- expand.grid(k=k_grid_search)

# Inner cross validation to determine the hyperparameters.
for(j in 1:n_features){
  tmp_feature_columns[j] <- rownames(rocVarImp_descending_ssq)[j]
  fit.knn <- train(Target~., 
                   data=train_data_array_df[,tmp_feature_columns[c(1:j,length(tmp_feature_columns))]], 
                   method="knn", 
                   metric=metric, 
                   trControl=control, 
                   tuneGrid = grid_knn)
  kNN_models[[j]] <- fit.knn
}

# Create a table with the best performing models for hyperparameter k and n features.
kNN_models[[1]]$results$n_features <- 1
performance_knn <- kNN_models[[1]]$results
best_knn_models <- kNN_models[[1]]$results[order(kNN_models[[1]]$results$Accuracy, decreasing = T),][1,]

for(j in 2:n_features){
  kNN_models[[j]]$results$n_features <- j
  performance_knn <- rbind(performance_knn, kNN_models[[j]]$results)
  tmp_best_model <- kNN_models[[j]]$results[order(kNN_models[[j]]$results$Accuracy, 
                                                  decreasing = T),][1,]
  best_knn_models <- rbind(best_knn_models, tmp_best_model)
}
best_knn_models <- best_knn_models[order(best_knn_models$Accuracy, decreasing = T),]

# Determine the best hyperparameters.
best_n_featues <- best_knn_models$n_features[1]
best_k <- best_knn_models$k[1]

# Make a final model with the best hyperparameters.
grid_knn <- expand.grid(k=best_k)

fit.knn <- train(Target~., 
                   data=train_data_array_df[,tmp_feature_columns[c(1:best_n_featues,length(tmp_feature_columns))]], 
                   method="knn", 
                   metric=metric, 
                   trControl=control, 
                   tuneGrid = grid_knn)

# Add the models of the outer loop to a list. i is in the format: 1, 21, 41, 61, 81. Change i to 1, 2, 3, 4, 5. Add also the prediction of the test set to a list.
kNN_models_outer[[floor(i / 10) / 2 + 1]] <- fit.knn
kNN_models_outer[[floor(i / 10) / 2 + 1]]$results$n_features <- best_n_featues
kNN_models_outer[[floor(i / 10) / 2 + 1]]$prediction <- confusionMatrix(predict(fit.knn, test_data_array_df), test_data_array_df$Target)


}
# Add the models of the n repeats to a list. n is in the format 0, 1, ..., 4. Change to 1, 2, ..., 5
kNN_models_outer_repeats[[n + 1]] <- kNN_models_outer
}

```

Evaluate the 300 kNN models
```{r}
knn_model_evaluation <- list(knn_evaluation = data.frame(k = rep(0, cv_outer * repeats_double_loop_cv),
                                   n_features = rep(0, cv_outer * repeats_double_loop_cv),
                                   Accuracy_train = rep(0, cv_outer * repeats_double_loop_cv),
                                   Accuracy_test = rep(0, cv_outer * repeats_double_loop_cv)), 
                             knn_mean_accuracy_train = 0,
                             knn_mean_accuracy_test = 0,
                             knn_mean_k = 0,
                             knn_mean_n_features = 0)

count <- 1
for(i in 1:repeats_double_loop_cv){
  for(j in 1:cv_outer){
    knn_model_evaluation$knn_evaluation[count,1] <- kNN_models_outer_repeats[[i]][[j]]$results$k
    knn_model_evaluation$knn_evaluation[count,2] <- kNN_models_outer_repeats[[i]][[j]]$results$n_features
    knn_model_evaluation$knn_evaluation[count,3] <- kNN_models_outer_repeats[[i]][[j]]$results$Accuracy
    knn_model_evaluation$knn_evaluation[count,4] <- kNN_models_outer_repeats[[i]][[j]]$prediction$overall[[1]]
    
    count <- count + 1
}}


knn_model_evaluation$knn_mean_accuracy_train <- mean(knn_model_evaluation$knn_evaluation$Accuracy_train)
knn_model_evaluation$knn_mean_accuracy_test <- mean(knn_model_evaluation$knn_evaluation$Accuracy_test)
knn_model_evaluation$knn_mean_k <- mean(knn_model_evaluation$knn_evaluation$k)
knn_model_evaluation$knn_mean_n_features <- mean(knn_model_evaluation$knn_evaluation$n_features)
knn_model_evaluation$knn_evaluation$k <- as.factor(knn_model_evaluation$knn_evaluation$k)

knn_model_evaluation
```

Make a final model based on the kNN evaluation. Use all the training data.
```{r}
train_data_array_df <- as.data.frame(select(train_data_array, -c(Instances)))
rownames(train_data_array_df) <- Instances
train_data_array_df$Target <- as.factor(train_data_array_df$Target)

grid_knn <- expand.grid(k=round(knn_model_evaluation$knn_mean_k))
final.knn <- train(Target~., 
                   data=train_data_array_df[,tmp_feature_columns[c(1:round(knn_model_evaluation$knn_mean_n_features),length(tmp_feature_columns))]], 
                   method="knn", 
                   metric=metric, 
                   trControl=control, 
                   tuneGrid = grid_knn)
```

Make a boxplot for 
```{r}
ggplot(data = knn_model_evaluation$knn_evaluation) +
  geom_boxplot(mapping = aes(x = k, y = n_features)) +
  geom_jitter(mapping = aes(x = k, y = n_features), width = 0.1) +
  scale_y_continuous(breaks = 1:n_features) +
  ggtitle("kNN") +
  theme_bw()

ggplot(data = knn_model_evaluation$knn_evaluation) +
  geom_bin2d(mapping = aes(x = k, y = n_features)) +
  ggtitle("kNN") +
  scale_y_continuous(breaks = 1:n_features)
```




## SVM polynomial
```{r}
repeats_double_loop_cv <-  20
svm_poly_models_outer_repeats <- vector(mode = "list", length = repeats_double_loop_cv)

for(n in 0:(repeats_double_loop_cv - 1)){

# Determine the number of outer cross validation loops
cv_outer <- 5

# Initiate a list for the outer loop models.
svm_poly_models_outer <- vector(mode = "list", length = cv_outer)

# Initialize the start and end test set indices.
test_index_start <- seq(1, dim(train_data_array)[1],)
test_index_end <- seq(dim(train_data_array)[1]/cv_outer, dim(train_data_array)[1] * 2)
test_index_list <- c(seq(1, dim(train_data_array)[1]), seq(1, dim(train_data_array)[1]))

# Outer cross-validation
for(i in seq(1, dim(train_data_array)[1], 20)){
# Define the test and training set in iteration i.
train_data_array_df <- as.data.frame(select(train_data_array, -c(Instances)))
rownames(train_data_array_df) <- Instances
train_data_array_df$Target <- as.factor(train_data_array_df$Target)

test_index <- test_index_list[test_index_start[i + n]:test_index_end[i + n]]

test_x_df <- train_data_array_df[test_index,] %>% select(-Target)
test_y_df <- train_data_array_df[test_index,] %>% select(Target)
test_data_array_df <- train_data_array_df[test_index,]

train_x_df <- train_data_array_df[-test_index,] %>% select(-Target)
train_y_df <- train_data_array_df[-test_index,] %>% select(Target)
train_data_array_df <- train_data_array_df[-test_index,]

# Rank the features on importance based on the filterVarImp function.
rocVarImp <- filterVarImp(train_x_df, train_y_df$Target)
rocVarImp$SSQ <- apply(rocVarImp[1:3], 1, function(x){sum(x^2)})

rocVarImp_descending_ssq <- rocVarImp[order(rocVarImp$SSQ, decreasing = T),]

# Define the model parameters. In general k is sqrt(n) where n is the number of data points. Meaning that 10 would be a reference.
set.seed(101)
n_features <- 12
tmp_feature_columns <- c(seq(1,n_features), "Target")
svm_poly_models <- vector(mode = "list", length = n_features)
control <- trainControl(method = "repeatedcv", number = 5, repeats = 5)
metric <- "Accuracy"
svm_poly_C <- c(0.001, 0.01, 0.05, 0.1,0.5, 1, 2)
svm_poly_sigma <- c(0.005, 0.01, 0.05, 0.1,0.5)
grid_svm_poly <- expand.grid(C = svm_poly_C, sigma = svm_poly_sigma)

# Inner cross validation to determine the hyperparameters.
for(j in 1:n_features){
  tmp_feature_columns[j] <- rownames(rocVarImp_descending_ssq)[j]
  fit.svm_poly <- train(Target~., 
                   data=train_data_array_df[,tmp_feature_columns[c(1:j,length(tmp_feature_columns))]], 
                   method="svmRadial", 
                   metric=metric, 
                   trControl=control, 
                   tuneGrid = grid_svm_poly)
  svm_poly_models[[j]] <- fit.svm_poly
}

# Create a table with the best performing models for hyperparameter k and n features.
svm_poly_models[[1]]$results$n_features <- 1
performance_svm_poly <- svm_poly_models[[1]]$results
best_svm_poly_models <- svm_poly_models[[1]]$results[order(svm_poly_models[[1]]$results$Accuracy, decreasing = T),][1,]

for(j in 2:n_features){
  svm_poly_models[[j]]$results$n_features <- j
  performance_svm_poly <- rbind(performance_svm_poly, svm_poly_models[[j]]$results)
  tmp_best_model <- svm_poly_models[[j]]$results[order(svm_poly_models[[j]]$results$Accuracy, 
                                                  decreasing = T),][1,]
  best_svm_poly_models <- rbind(best_svm_poly_models, tmp_best_model)
}
best_svm_poly_models <- best_svm_poly_models[order(best_svm_poly_models$Accuracy, decreasing = T),]

# Determine the best hyperparameters.
best_C <- best_svm_poly_models$C[1]
best_sigma <- best_svm_poly_models$sigma[1]
best_n_featues <- best_svm_poly_models$n_features[1]
# Make a final model with the best hyperparameters.
grid_svm_poly <- expand.grid(C = best_C, sigma = best_sigma)

fit.svm_poly <- train(Target~., 
                   data=train_data_array_df[,tmp_feature_columns[c(1:best_n_featues,length(tmp_feature_columns))]], 
                   method="svmRadial", 
                   metric=metric, 
                   trControl=control, 
                   tuneGrid = grid_svm_poly)

# Add the models of the outer loop to a list. i is in the format: 1, 21, 41, 61, 81. Change i to 1, 2, 3, 4, 5. Add also the prediction of the test set to a list.
svm_poly_models_outer[[floor(i / 10) / 2 + 1]] <- fit.svm_poly
svm_poly_models_outer[[floor(i / 10) / 2 + 1]]$results$n_features <- best_n_featues
svm_poly_models_outer[[floor(i / 10) / 2 + 1]]$prediction <- confusionMatrix(predict(fit.svm_poly, test_data_array_df), test_data_array_df$Target)


}
# Add the models of the n repeats to a list. n is in the format 0, 1, ..., 4. Change to 1, 2, ..., 5
svm_poly_models_outer_repeats[[n + 1]] <- svm_poly_models_outer
}

```


```{r}
svm_poly_model_evaluation <- list(svm_poly_evaluation = data.frame(C = rep(0, cv_outer * repeats_double_loop_cv),
                                   Sigma = rep(0, cv_outer * repeats_double_loop_cv),                             
                                   n_features = rep(0, cv_outer * repeats_double_loop_cv),
                                   Accuracy_train = rep(0, cv_outer * repeats_double_loop_cv),
                                   Accuracy_test = rep(0, cv_outer * repeats_double_loop_cv)), 
                             svm_poly_mean_accuracy_train = 0,
                             svm_poly_mean_accuracy_test = 0,
                             svm_poly_mean_C = 0,
                             svm_poly_mean_sigma = 0,
                             svm_poly_mean_n_features = 0)

count <- 1
for(i in 1:repeats_double_loop_cv){
  for(j in 1:cv_outer){
    svm_poly_model_evaluation$svm_poly_evaluation[count,1] <- svm_poly_models_outer_repeats[[i]][[j]]$results$C
    svm_poly_model_evaluation$svm_poly_evaluation[count,2] <- svm_poly_models_outer_repeats[[i]][[j]]$results$sigma
    svm_poly_model_evaluation$svm_poly_evaluation[count,3] <- svm_poly_models_outer_repeats[[i]][[j]]$results$n_features
    svm_poly_model_evaluation$svm_poly_evaluation[count,4] <- svm_poly_models_outer_repeats[[i]][[j]]$results$Accuracy
    svm_poly_model_evaluation$svm_poly_evaluation[count,5] <- svm_poly_models_outer_repeats[[i]][[j]]$prediction$overall[[1]]
    
    count <- count + 1
}}

svm_poly_model_evaluation$svm_poly_mean_accuracy_train <- mean(svm_poly_model_evaluation$svm_poly_evaluation$Accuracy_train)
svm_poly_model_evaluation$svm_poly_mean_accuracy_test <- mean(svm_poly_model_evaluation$svm_poly_evaluation$Accuracy_test)
svm_poly_model_evaluation$svm_poly_mean_C <- mean(svm_poly_model_evaluation$svm_poly_evaluation$C)
svm_poly_model_evaluation$svm_poly_mean_sigma <- mean(svm_poly_model_evaluation$svm_poly_evaluation$Sigma)
svm_poly_model_evaluation$svm_mean_n_features <- mean(svm_poly_model_evaluation$svm_poly_evaluation$n_features)

svm_poly_model_evaluation
```


```{r}
train_data_array_df <- as.data.frame(select(train_data_array, -c(Instances)))
rownames(train_data_array_df) <- Instances
train_data_array_df$Target <- as.factor(train_data_array_df$Target)

grid_svm_poly <- expand.grid(C = svm_poly_model_evaluation$svm_poly_mean_C, sigma = svm_poly_model_evaluation$svm_poly_mean_sigma)
final.svm_poly <- train(Target~., 
                   data=train_data_array_df[,tmp_feature_columns[c(1:round(svm_poly_model_evaluation$svm_mean_n_features),length(tmp_feature_columns))]], 
                   method="svmRadial", 
                   metric=metric, 
                   trControl=control, 
                   tuneGrid = grid_svm_poly)
```

```{r}
svm_poly_model_evaluation$svm_poly_evaluation$C <- as.factor(svm_poly_model_evaluation$svm_poly_evaluation$C)
svm_poly_model_evaluation$svm_poly_evaluation$Sigma <- as.factor(svm_poly_model_evaluation$svm_poly_evaluation$Sigma)
svm_poly_model_evaluation$svm_poly_evaluation$n_features <- as.factor(svm_poly_model_evaluation$svm_poly_evaluation$n_features)

ggplot(data = svm_poly_model_evaluation$svm_poly_evaluation) +
  geom_bar(mapping = aes(x = C), fill = "Blue") +
  ggtitle("SVM") +
  theme_bw()

ggplot(data = svm_poly_model_evaluation$svm_poly_evaluation) +
  geom_bar(mapping = aes(x = Sigma), fill = "Blue") +
  ggtitle("SVM") +
  theme_bw()

ggplot(data = svm_poly_model_evaluation$svm_poly_evaluation) +
  geom_bar(mapping = aes(x = n_features), fill = "Blue") +
  ggtitle("SVM") +
  theme_bw()
```



